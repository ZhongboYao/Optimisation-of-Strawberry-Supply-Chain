{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55735bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "from Read_Data import Read_data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import Plots\n",
    "import numpy as np\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7198e05",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dec051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(Dataset):\n",
    "  '''\n",
    "  Format the data of features and labels using Datset for the prupose of being splited and\n",
    "  seperated into batches.\n",
    "  '''\n",
    "  def __init__(self, history: torch.tensor, future:torch.tensor, transform=None, target_transform=None):\n",
    "    self.history = history\n",
    "    self.future = future\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.future)\n",
    "\n",
    "  def __getitem__(self, idx:int) -> tuple[int, int]: \n",
    "    history_item = self.history[idx]\n",
    "    future_item = self.future[idx]\n",
    "    if self.transform:\n",
    "      history_item = self.transform(history_item)\n",
    "    if self.target_transform:\n",
    "      future_item = self.target_transform(future_item)\n",
    "    return history_item, future_item"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cccdfe5",
   "metadata": {},
   "source": [
    "## Definition of the LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e431a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNET(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size: int, hidden_size: int, num_layers: int, num_classes: int, seq_length: int, gear=None):\n",
    "    '''\n",
    "    input_size: how many kinds of features are in the input.\n",
    "    hidden_size: hyper-parameter\n",
    "    num_layers: hyper-parameter\n",
    "    num_classes: the number of outputs each time.\n",
    "    seq_length: the length of the input, which should be equal to the length of the sliding window.\n",
    "    gear: the number of fc layers in the neural network +1 \n",
    "          (because there should at least be one fc layer to reshape the outputs).\n",
    "    '''\n",
    "    super(LSTMNET, self).__init__()\n",
    "\n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.num_classes = num_classes\n",
    "    self.seq_length = seq_length\n",
    "    self.gear = gear\n",
    "    self.fc_layers = []\n",
    "    \n",
    "\n",
    "    self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "    self.sig = nn.Sigmoid()\n",
    "    if gear == None: # Default configuration.\n",
    "      self.fc1 = nn.Linear(self.seq_length * self.hidden_size, self.hidden_size)\n",
    "      self.fc2 = nn.Linear(self.hidden_size, int(self.hidden_size/2))\n",
    "      self.fc3 = nn.Linear(int(self.hidden_size/2), self.num_classes)\n",
    "\n",
    "    else:\n",
    "      start_size = self.seq_length * self.hidden_size\n",
    "      end_size = self.hidden_size\n",
    "      if gear == 0:\n",
    "        self.fc_layers.append(nn.Linear(start_size, self.num_classes))\n",
    "      else:\n",
    "        for i in range(gear):\n",
    "          self.fc = nn.Linear(start_size, end_size)\n",
    "          self.fc_layers.append(self.fc)\n",
    "          start_size = end_size\n",
    "          end_size = end_size // 2\n",
    "          if end_size <= self.num_classes:\n",
    "            break\n",
    "        self.fc_layers.append(nn.Linear(start_size, self.num_classes))\n",
    "\n",
    "    self.trainer = None\n",
    "    self.test_results = []\n",
    "\n",
    "  def forward(self, input: torch.tensor):\n",
    "    h_0 = Variable(torch.rand(self.num_layers, input.shape[0], self.hidden_size)).to(self.device)\n",
    "    c_0 = Variable(torch.rand(self.num_layers, input.shape[0], self.hidden_size)).to(self.device)\n",
    "    lstm_n, (h_n, c_n) = self.lstm(input, (h_0, c_0))  \n",
    "    y_n = lstm_n.reshape(lstm_n.shape[0], -1)\n",
    "    if self.gear == None:\n",
    "      y_n = self.fc1(y_n)\n",
    "      y_n = self.sig(y_n)\n",
    "      y_n = self.fc2(y_n)\n",
    "      y_n = self.sig(y_n)\n",
    "      y = self.fc3(y_n)\n",
    "\n",
    "    else:\n",
    "      for i in range(self.gear):\n",
    "        y_n = self.fc_layers[i](y_n)\n",
    "        y_n = self.sig(y_n)\n",
    "      y = self.fc_layers[-1](y_n)\n",
    "    return y\n",
    "\n",
    "  def training_step(self, batch: torch.tensor) -> float:\n",
    "    '''\n",
    "    One epoch in training.\n",
    "    This function generates the prediction using the model trained so fat an then generates the loss.\n",
    "    '''\n",
    "    for i in range(len(batch)):\n",
    "        batch[i] = batch[i].float() # Input should be the type of float64.\n",
    "    loss = self.loss(self(*batch[:-1]), batch[-1]) # Predict and generate the loss at the same time.\n",
    "    return loss\n",
    "  \n",
    "  def validation_step(self, batch: torch.tensor) -> float:\n",
    "    for i in range(len(batch)):\n",
    "        batch[i] = batch[i].float()\n",
    "    loss = self.loss(self(*batch[:-1]), batch[-1])\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, batch: torch.tensor) -> float:\n",
    "    '''\n",
    "    Calculate the test loss and append the prediction results.\n",
    "    '''\n",
    "    for i in range(len(batch)):\n",
    "        batch[i] = batch[i].float()\n",
    "    outputs = self(*batch[:-1])\n",
    "    loss = self.loss(outputs, batch[-1])\n",
    "    self.test_results.append(outputs)\n",
    "    return loss\n",
    "\n",
    "  def prediction_step(self, window: torch.tensor) -> torch.tensor:\n",
    "    inputs = torch.from_numpy(window.astype('float64')).to(self.device)\n",
    "    inputs = torch.unsqueeze(inputs, dim=0)\n",
    "    inputs = inputs.float()\n",
    "    return self(inputs)\n",
    "\n",
    "  def loss(self, y_hat: torch.tensor, y: torch.tensor) -> float:\n",
    "    l = nn.MSELoss()\n",
    "    return l(y_hat, y)\n",
    "    \n",
    "  def configure_optimizers(self, lr: float) -> optim:\n",
    "    '''\n",
    "    Configure the hyper-parameters for the adam optimiser.\n",
    "    '''\n",
    "    return optim.Adam(self.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebdf4fee",
   "metadata": {},
   "source": [
    "## Training, Validation, Test and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b89f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, name: str, model: nn.Module, DataModule: DataModule, read_data: Read_data, max_epoch: int, lr: float, patience: int, warm: int):\n",
    "    '''\n",
    "    name: The name of the predictor, should be identical to the name of the input dataclass.\n",
    "    model: neural network class.\n",
    "    DataModule: DataModule class.\n",
    "    read_data: The dataclass initialised by Read_data.\n",
    "    max_epoch: Maximum number of epochs for training the neural network.\n",
    "    lr: Learning rate.\n",
    "    Patience: After num=patience epochs the training process stops.\n",
    "    warm: The early stopping mechanism starts after num=warm epochs.\n",
    "    '''\n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.name = name\n",
    "    self.model = model.to(self.device)\n",
    "    self.DataModule = DataModule\n",
    "    self.read_data = read_data\n",
    "    self.max_epoch = max_epoch\n",
    "    self.lr = lr\n",
    "    self.patience = patience\n",
    "    self.warm = warm\n",
    "\n",
    "    self.best_val_loss = float('inf')\n",
    "    self.patience_counter = 0\n",
    "\n",
    "    self.features = read_data.history_t\n",
    "    self.labels = read_data.future_t\n",
    "\n",
    "    self.model.trainer = None\n",
    "    self.optimiser = None\n",
    "    self.train_loader = None\n",
    "    self.val_loader = None\n",
    "    self.test_loader = None   \n",
    "    self.loss_train = []\n",
    "    self.loss_val = []\n",
    "    self.loss_test = []\n",
    "    self.test_truth = []\n",
    "    self.prediction_results = []\n",
    "    \n",
    "  def _prepare(self, portion_val: float, portion_test: float, batch_size: int):\n",
    "    '''\n",
    "    Split the dataset into trainset, validation set and testset, and format them into batches.\n",
    "    self.train_loader, self.val_set and self.test_loader are used to store the sets after split.\n",
    "    \n",
    "    portion_val: Split ratio for the validation set.\n",
    "    portion_test: Split ratio for the test set.\n",
    "    batch_size: batch size.\n",
    "    '''\n",
    "    dataset = self.DataModule(self.features, self.labels)\n",
    "    val_len = int(dataset.__len__() * portion_val)\n",
    "    test_len = int(dataset.__len__() * portion_test)\n",
    "    train_len = dataset.__len__() - val_len - test_len\n",
    "\n",
    "    train_data = torch.utils.data.Subset(dataset, range(0, train_len))\n",
    "    val_data = torch.utils.data.Subset(dataset, range(train_len, train_len + val_len))\n",
    "    test_data = torch.utils.data.Subset(dataset, range(train_len + val_len, len(dataset)))\n",
    "    \n",
    "    self.train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    self.val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    self.test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    self.model.trainer = self\n",
    "    \n",
    "  def fit(self, portion_val: float, portion_test: float, batch_size: int):\n",
    "    '''\n",
    "    Train the model until the early stopping. \n",
    "    The function calls the _prepare() automatically.\n",
    "    '''\n",
    "    self._prepare(portion_val, portion_test, batch_size)\n",
    "    self.optim = self.model.configure_optimizers(self.lr)\n",
    "\n",
    "    # with tqdm(total = self.max_epoch) as pbar:\n",
    "    for epoch in range(self.max_epoch):\n",
    "      # pbar.update(1)\n",
    "      loss = self.fit_epoch()\n",
    "      if epoch >= self.warm:\n",
    "        if loss < self.best_val_loss:\n",
    "          self.best_val_loss = loss\n",
    "          self.patience_counter = 0\n",
    "        else:\n",
    "          self.patience_counter += 1\n",
    "\n",
    "        if self.patience_counter >= self.patience:\n",
    "          return\n",
    "          \n",
    "  def fit_epoch(self) -> float:\n",
    "    '''\n",
    "    The training and validation process in one epoch.\n",
    "\n",
    "    return: The average validation loss for early stopping.\n",
    "    '''\n",
    "    # Training.\n",
    "    self.model.train()\n",
    "    loss_train_tmp = 0\n",
    "    for batch in self.train_loader:\n",
    "      batch = [item.to(self.device) for item in batch]\n",
    "      loss = self.model.training_step(batch).cpu()\n",
    "      loss_train_tmp += loss.detach()\n",
    "      self.optim.zero_grad()\n",
    "      with torch.no_grad():\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "    self.loss_train.append(loss_train_tmp/len(self.train_loader))\n",
    "\n",
    "    # Validation.\n",
    "    if self.val_loader is None:\n",
    "      return\n",
    "\n",
    "    self.model.eval()\n",
    "    loss_val_tmp = 0\n",
    "    for batch in self.val_loader:\n",
    "      batch = [item.to(self.device) for item in batch]\n",
    "      with torch.no_grad():\n",
    "        loss_val_tmp += self.model.validation_step(batch).cpu()\n",
    "    avg_loss_val = loss_val_tmp/len(self.val_loader)\n",
    "    self.loss_val.append(avg_loss_val)\n",
    "    return avg_loss_val\n",
    "      \n",
    "  def test(self): \n",
    "    '''\n",
    "    Test.\n",
    "    '''\n",
    "    self.model.eval()\n",
    "    \n",
    "    for batch in self.test_loader:\n",
    "      batch = [item.to(self.device) for item in batch]\n",
    "      self.test_truth.append(batch[-1])\n",
    "      with torch.no_grad():\n",
    "        loss = self.model.test_step(batch).cpu()\n",
    "        self.loss_test.append(loss)\n",
    "\n",
    "  def run(self, portion_val: float, portion_test, batch_size: int):\n",
    "    '''\n",
    "    Train, validate and test the LSTM network with calling the fit() and test() functions.   \n",
    "    name: The name of this network.\n",
    "    '''\n",
    "    \n",
    "    self.fit(portion_val, portion_test, batch_size)\n",
    "    self.test()\n",
    "\n",
    "  def test_plot(self):\n",
    "    '''\n",
    "    Plot two lines comparing the truth and predictions.\n",
    "    The predictions are from self.test_results and the truth is from the test_loader.\n",
    "    '''\n",
    "    y_hat = torch.cat(self.model.test_results).cpu().numpy().reshape(1, -1)\n",
    "    y = torch.cat(self.test_truth).cpu().numpy().reshape(1, -1)\n",
    "    Plots.plot_lines(np.array([y_hat, y]).reshape(2, -1),\n",
    "                     title='Truth VS Prediction of '+self.name+' LSTM', xlabel='Weeks', ylabel='values', label=['Prediction', 'Truth'], show_label=1)\n",
    "\n",
    "  def predict(self):\n",
    "    '''\n",
    "    Predict the data.\n",
    "    The number of the months predicted is equal to the read_data.predict_length.\n",
    "    Finally, the predictions are transferred from torch.tensor to numpy numbers.\n",
    "    '''\n",
    "    self.model.eval()\n",
    "    predictions_tmp = []\n",
    "    with torch.no_grad():\n",
    "      for time_step in range(self.read_data.predict_length):\n",
    "        feature = self.read_data.data_normalised[-(self.read_data.window+self.read_data.window_interval)+time_step : -(self.read_data.window_interval)+time_step, 0].reshape(-1, 1)\n",
    "        output = self.model.prediction_step(feature)\n",
    "        predictions_tmp.append(self.read_data.scaler.inverse_transform(output.numpy()).reshape(-1)) # Reverse normalisation to get data in the real scale.\n",
    "    self.prediction_results = [x.item() for x in predictions_tmp]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb12d212",
   "metadata": {},
   "source": [
    "## Execution Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a829aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUL_DEMAND = 1500\n",
    "# MUL_PRICE = 1000/340.2\n",
    "# MUL_YIELD = 2000\n",
    "# TRUE_WEEKS = 12\n",
    "# MAX_EPOCH = 10000\n",
    "\n",
    "# hyper = {'window':          {'Demand': 17,      'Price': 12,     'Yield': 15    },\n",
    "#          'hs':              {'Demand': 24,      'Price': 10,     'Yield': 16    },\n",
    "#          'num_layers':      {'Demand': 1,       'Price': 2,      'Yield': 2     },\n",
    "#          'lr':              {'Demand': 0.00005, 'Price': 0.0001, 'Yield': 0.0001},\n",
    "#          'patience':        {'Demand': 1000,    'Price': 2000,   'Yield': 10000 },\n",
    "#          'warm':            {'Demand': 500,     'Price': 200,    'Yield': 200   },\n",
    "#          'portion_val':     {'Demand': 0.1,     'Price': 0.1,    'Yield': 0.1   },\n",
    "#          'portion_test':    {'Demand': 0.1,     'Price': 0.1,    'Yield': 0.1   },\n",
    "#          'batch_size':      {'Demand': 128,     'Price': 128,    'Yield': 128   }\n",
    "#         }\n",
    "\n",
    "# demand = Read_data('../Strawberry Demand.csv', 'Demand', true_weeks=TRUE_WEEKS, mul=MUL_DEMAND, window=hyper['window']['Demand'])\n",
    "# price = Read_data('../Strawberry Price.csv', 'Price', true_weeks=TRUE_WEEKS, mul=MUL_PRICE, window=hyper['window']['Price'])\n",
    "# syield = Read_data('../Yield.csv', 'Yield', true_weeks=TRUE_WEEKS, mul=MUL_YIELD, window=hyper['window']['Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b19c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neural_network_exe(data: Read_data, plot=0, gear=None) -> tuple:\n",
    "#     '''\n",
    "#     Execution of the training, validation and test of the neulra network.\n",
    "#     data: data class that is used as the prediction target.\n",
    "#     plot: choose whether to plot the results, binary.\n",
    "#     '''\n",
    "#     name = data.name\n",
    "#     model = LSTMNET(input_size=1, hidden_size=hyper['hs'][name], num_layers=hyper['num_layers'][name], num_classes=1, seq_length=hyper['window'][name], gear=gear)\n",
    "#     trainer = Trainer(name, model, DataModule, data, MAX_EPOCH, hyper['lr'][name], hyper['patience'][name], hyper['warm'][name])\n",
    "#     trainer.fit(hyper['portion_val'][name], hyper['portion_test'][name], hyper['batch_size'][name])\n",
    "#     trainer.test()\n",
    "#     trainer.predict()\n",
    "\n",
    "#     if plot == 1:\n",
    "#         Plots.plot_lines(np.array(trainer.loss_train).reshape(1, -1), title='Training Loss of ' + str(trainer.name) + ' LSTM', xlabel='Epochs', ylabel='MSE Loss', label=['Training Loss'], show_label=1)\n",
    "#         Plots.plot_lines(np.array(trainer.loss_val).reshape(1, -1), title='Validation Loss of ' + str(trainer.name) + ' LSTM', xlabel='Epochs', ylabel='MSE Loss', label=['Validation Loss'], show_label=1)\n",
    "#         trainer.test_plot()\n",
    "#         print('Train loss of the ' + str(trainer.name) + ' LSTM: ' + str(trainer.loss_train[-1]))\n",
    "#         print('Validation loss of the ' + str(trainer.name) + ' LSTM: ' + str(trainer.loss_val[-1]))\n",
    "    \n",
    "#     return trainer.loss_train, trainer.loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01a43fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in [demand, syield, price]:\n",
    "#     _, _ = neural_network_exe(syield, plot=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8fbe442",
   "metadata": {},
   "source": [
    "## Structure Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb95affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss_record = []\n",
    "\n",
    "# for gear in range(5):\n",
    "#     valLoss_total = 0\n",
    "#     for _ in range(10):\n",
    "#         _, val_loss = neural_network_exe(demand, plot=0, gear=gear)\n",
    "#         valLoss_total += val_loss[-1]\n",
    "#     val_loss_record.append(valLoss_total/3)\n",
    "\n",
    "# Plots.plot_lines(np.array(val_loss_record).reshape(1, -1), x=np.arange(1, 6, 1), xticks=np.arange(1, 6, 1),\n",
    "#                  title='Loss VS FC Layers Number', xlabel='The Number of FC Layers', ylabel='Validation Loss', label=['validation loss'], show_label=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5334a7a6",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9b2d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# for window in np.arange(15, 25):\n",
    "#     demand = Read_data('../Strawberry Demand.csv', 'Demand', true_weeks=TRUE_WEEKS, mul=MUL_DEMAND, window=window)\n",
    "#     name = demand.name\n",
    "#     model = LSTMNET(input_size=1, hidden_size=hyper['hs'][name], num_layers=hyper['num_layers'][name], num_classes=1, seq_length=window)\n",
    "#     trainer = Trainer(name, model, DataModule, demand, MAX_EPOCH, hyper['lr'][name], hyper['patience'][name], hyper['warm'][name])\n",
    "#     trainer.fit(hyper['portion_val'][name], hyper['portion_test'][name], hyper['batch_size'][name])\n",
    "#     trainer.test()\n",
    "#     trainer.predict()\n",
    "#     losses.append(trainer.loss_val[-1])\n",
    "# Plots.plot_lines(np.array(losses).reshape(1, -1), x=np.arange(15, 25),\n",
    "#                  title='Loss VS Window Length', xlabel='Window', ylabel='Validation Loss', label=['validation loss'], show_label=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
